%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{url}
\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text
\def \cov {\text{Cov}}
\def \var {\text{Var}}

\title{STATS 116: Theory of Probability}
%A First Course in Probability, Ross.
\author{sogapalag}

\date{\normalsize\today}

\begin{document}

\maketitle

\section{Combinatorial Analysis}
The generalized basic principle of counting.\\
permutations\\
The binomial theorem\\
$n$ distinct items to $r$ distinct groups with size $\sum n_r = n$:
\begin{equation}
	{n \choose {n_1,...,n_r}} = \frac{n!}{n_1!\dots n_r!}
\end{equation}
The multinomial theorem\\
$\sum_r x_j = n, x_j>0$, number of solution (distinct integer-valued) vectors $v=(x_j,..)$, is ${n-1 \choose r-1}$.\\
if $x_j\geq 0$, number is ${n+r-1 \choose r-1}$.\\
n choose k with repetition, number is ${n+k-1 \choose n-1}$.\\
distinct $n$ items distribute to $k$ distinct groups, $(1+1+\dots+1)^n=k^n$, i.e. every items's group choice always $k$, the formula tells, there is ${n+k-1 \choose k-1}$ terms, each term corresponding to a ${n \choose {n_1,...,n_k}}$.\\
\begin{align}
	{n\choose k } &= \frac{n}{k} {n-1\choose k-1}\\
	&= {n-1 \choose k-1} + {n-1 \choose k} \\
	&= {n+1 \choose k+1} - {n\choose k+1}
\end{align}

\section{Axioms of Probability}
Distributive laws, $(E\cup F) G = EG\cup FG$, $EF\cup G = (E\cup G)(F\cup G)$\\
%样本空间的并（交）操作 对偶于 对偶空间的交（并）操作；即当作一个操作时，同时对偶空间也在操作。
DeMorgan's laws:
\begin{align}
	\left( \bigcup E_i \right)^c &= \bigcap E_i^c \\
	\left( \bigcap E_i \right)^c &= \bigcup E_i^c
\end{align}
Axiom 1, $0\leq P(E) \leq 1$;\\
Axiom 2, $P(S)=1$;\\
Axiom 3, $\forall E_iE_j = \emptyset,i\neq j$, $P(\bigcup_{i=1}^\infty E_i) = \sum_{i=1}^\infty P(E_i)$.\\
Axiom 3 implies $P(\emptyset) =0$.\\
Propositions:
\begin{align}
P(\bigcup E_i) = \sum_{r=1}^n (-1)^{r+1} \sum_{i_1<\dots<i_r} P(E_{i_1}\dots E_{i_r}) 
\end{align}
if sequence $E_n$ monotic, then $\lim P(E_n) = P(\lim E_n)$.\\

\section{Conditional Probability and Independence}
Def, $P(E|F) = \frac{P(EF)}{P(F)}$.\\
The multiplication rule,
\begin{align}
	P(E_1E_2\dots E_n) = P(E_1)P(E_2|E_1)P(E_3|E_1E_2)\dots P(E_n|E_1\dots E_{n-1})
\end{align}
def, odds $P(A)/P(A^c)$.\\
independent, $P(EF)=P(E)P(F)$.\\
prop, if E and F independent, so E and $F^c$.\\
mutually exclusive $F_i$, and $\sum P(F_i)=1$, then
\begin{align}
P(F_j|E) = \frac{P(E|F_j)P(F_j)} {\sum_i P(E|F_i)P(F_i)}
\end{align}
Laplace's rule of succession.\\

\section{Random Variables}
distribution function $F(x) = P\{X\leq x\}$.\\
countable number of possible value, discrete, $\sum_i p(x_i) = 1$.\\
If discrete, $F$ is step function.\\
expected value,(mean, first moment) $E[X] = \sum_{x:p(x)>0} xp(x)$. analogous to center of gravity\\
prop, $X$ is discrete random varible respect to $p$, for any real-valued $g$, $E[g(X)]= \sum_i g(x_i)p(x_i)$.\\
corollary, $E[aX+b] =aE[X]+b$.\\
variance $\text{Var}(X)=E[(X-\mu)^2] = E[X^2] - \mu^2$.\\
$\text{Var}(aX+b) = a^2 \text{Var}(X)$.\\
standard deviation, $\text{SD}(X) = \sqrt{\text{Var}(X)}$.\\
Bernoulli random variable, special case $(1,p)$ of binomial random variable $(n,p)$. $X$ represents the number of successes occur in the $n$ trials, $p(i)={n\choose i} p^i (1-p)^{n-i}$.\\
properties of Bonomial Random Variables, $E[X^k] = np E[(Y+1)^{k-1}]$, thus $E[X]=np$, $E[X^2] = np[(n-1)p+1]$, $\text{Var}(X) = np(1-p)$.\\
prop, binomial, increasing monotonically then decreasing monotonically, reaching its largest value when $k=\lfloor (n+1)p \rfloor$.\\
Poisson random variable, $p(i) = e^{-\lambda} \frac{\lambda^i}{i!}, i=0,1,2,...$; approximation for Binomial $(n,p)$ when $n$ is large, $p$ is small enough, let $\lambda=np$. $E[X]=\lambda$, $\text{Var}(X)=\lambda$.\\
Based on 3 assumptions: probability exactly 1 event occurs in a given interval length $h$ is, $\lambda h+o(h)$, i.e. $\lim_{h\rightarrow 0}o(h)/h = 0$; probability that 2 or more events occur in an interval os length $h$ is, $o(h)$; For any interger $n$, nonoverlapping interval $j_1,...,j_n$, event occur $E_i$ be occur in $j_i$, which are independent. Then number of occur in interval of length $t$ is Poisson random variable with mean $\lambda t$.\\
%total r successes, require trials n.
geometric random variable, special case $(1,p)$ of negative binomial random variable $(r,p)$. $P\{X=n\} = {n-1\choose r-1} p^r(1-p)^{n-r}, n=r,r+1,...$; $E[X^k] = \frac{r}{p}E[(Y-1)^{k-1}]$, $E[X]=r/p$, $E[X^2]=\frac{r}{p}(\frac{r+1}{p}+1)$, $\text{Var}(X)=\frac{r(1-p)}{p^2}$.\\
%N+1 succ, 2N-k+1 trials, nega bino.
The Banach match problem\\
hypergeometric random variable,
\begin{align}
	&P\{X=i\} = \frac{{m\choose i}{N-m\choose n-i}}{{N\choose n}}, i=0,1,...,n\\
	&E[X^k] = \frac{nm}{N} E[(Y+1)^{k-1}]\\
	&E[X] = \frac{nm}{N}\\
	&E[X^2] = \frac{nm}{N}\left[\frac{(n-1)(m-1)}{N-1}+1\right]\\
	&\text{Var}(X) = np(1-p)(1 - \frac{n-1}{N-1}),\ \text{let $p=m/N$}
\end{align}
when $N$ is large compare to $n$, approximation for binomial.\\
zeta distribution, $P\{X=k\} = \frac{C}{k^{\alpha +1}}, k=1,2,...$.\\
prop, $E[X]=\sum_{s\in S} X(s)p(s)$.\\
corollary, $E[\sum X_i] = \sum E[X_i]$.\\

\section{Continuous Random Variables}
probability density function.\\
prop, with real-valued $g$, that $E[g(X)] = \int_{-\infty}^\infty g(x)f(x)dx$.\\
%for dy, scaned length y.
lemma, nonnegative $Y$, $E[Y]=\int_0^\infty P\{Y>y\}dy$.\\
Corollary, $E[aX+b] =aE[X]+b$, $\text{Var}(aX+b)=a^2\text{Var}(X)$.\\
uniformly distributed, $\text{Var}(X)=(b-a)^2/12$.\\
Bertrand's paradox, should define what kind of random.\\
normal distribution. $(\mu,\sigma^2)$. standard $Z =(X-\mu)/\sigma$\\
The DeMoivre-Laplace limit theorem, let  $S_n$ denotes the number of successes occur when $n$ independent trials, each probability $p$, i.e. binomial $(n,p)$. Then for any $a<b$, as $n\rightarrow \infty$:
\begin{align}
	P\left\{a\leq \frac{S_n-np}{\sqrt{np(1-p)}}\leq b\right\}\rightarrow \Phi(b) - \Phi(a)
\end{align}
Which is special case of central limit theorem. Recall $n$ is large, $p$ is small enough, approx Poisson.\\
%e.g. Half-life
exponential random variable. a key property memoryless. $P\{X>s+t|X>t\} = P\{X>s\}$.\\
Hazard Rate Functions\\
%exponential is special gamma with alpha=1
gamma distribution, $(\alpha,\lambda)$, $\lambda>0$.\\
The Weibull Distribution\\
The Cauchy Distribution\\
The Beta Distribution, note $B(a,b)=\Gamma(a)\Gamma(b)/\Gamma(a+b)$.\\
Thm, suppose $g(x)$ strictly monotonic(imply bijective), differentiable, the pdf of $Y=g(x)$ given by
\begin{align}
	f_Y(y)=\begin{cases} f_X[g^{-1}(y)]\left|\frac{d}{dy}g^{-1}(y)\right| & \mbox{if $y=g(x)$}\\
				0 & \mbox{if $y\neq g(x)$}\end{cases}
\end{align}

\section{Jointly Distributed Random Variables}
jointly continuous, joint probability density function\\
%独立，pdf在R上可拆分
independent random variables, for all set A, B, independent. iff joint pdf can separate on $R$, i.e. $f_{X,Y}(x,y) = h(x)g(y), -\infty<x,y<\infty$.\\
%独立，是对称的
Independence is a symmetric relation. We can find a easy way to argue X independent given Y or Y independent given X.\\
Corollay, we can sequentially show independence of $X_1,...,X_n$, i.e. show $X_r$ independent of $X_1,...,X_{r-1}, r>1$.\\
%卷积
convolution, independent $X,Y$ then
\begin{align}
	 f_{X+Y}(a) = \int_{-\infty}^\infty f_X(a-y)f_Y(y)dy
\end{align}
%期望e个uniform数的和才超过1
sum of independent uniform random variables. $E[N]=e$ sum up to exceed 1.\\
%独立分布的和
%since a has the form of gamma(a)
prop, if $X,(s,\lambda)$ and $Y,(t,\lambda)$ independent gamma rvs, then $X+Y$ is gamma rv with parameters $(s+t,\lambda)$.\\
prop, if $X_i$ are indpendent normal distribution with $(\mu_i,\sigma_i^2)$, then $\sum X_i$ is normal distribution with $(\sum \mu_i, \sum \sigma_i^2)$.\\
lognormal rv $Y=e^X$, where $X$ is normal rv.\\
if $X,Y$ are independent Poisson rv, wrto $\lambda_1,\lambda_2$, then $X+Y$ is Poisson with $\lambda_1+\lambda_2$.\\
if $X,Y$ are independent binomial rv, wrto $(n,p),(m,p)$, trivially by def of binomial, $X+Y$ is binomial with $(n+m,p)$.\\
for geometric rv $X_i$ with $p_i$, if all $p_i$ equals, trivially $\sum X_i$ becomes negative binomial rv.\\
%条件分布
conditional pdf $p_{X|Y}(x|y) = \frac{p(x,y)}{p_Y(y)}$.\\
%给定Poisson分布和，元分布为binomial分布
if $X,Y$ independent Poisson rv with $\lambda_1,\lambda_2$, then conditional distribution of X given $X+Y=n$, is a binomial distribution with $n,p=\lambda_1/(\lambda_1+\lambda_2)$.\\
%去掉一些输出，仍为多项式分布
multinomial distribution, $\sum p_i=1$, given some outcome not occur, then probability of $i$ occur is percentage of left. i.e. multinomial distribution of remains.\\
Bivariate Normal Distribution\\
%since the bino form x^n(1-x)^m, x vary is beta form.
%binomial先验概率为uniform的，给定成功次数，后验概率为beta分布
Consider $n+m$ trials with common probability uniformly distributed, given $n$ success, then conditional distribution of success probability is beta distribution.\\
%独立全同分布，可视为排序因子n!然后直接考虑有序情况
order statistics, $X_i$ independent and identically with continuous $f$ and $F$, then order statistics with pdf $f_{X_{(i)...}} (x_i,...) = n! f(x_i)\dots$, i.e. we can simply multiply permutation, then consider an ordered rvs.\\
%独立全同uniform分布的差(range)为beta分布
range $R=X_{(n)} - X_{(1)}$ distribution, of uniform rv, is beta rv with parameter $n-1,2$.\\
Jacob, function distribution.\\
$X,Y$ standard normal, into $r,\theta$, $f(r,\theta)=\frac{1}{2\pi} re^{-r^2/2}$, i.e. angle uniform distributed and independent to distance $r$.\\
%gamma分布比例为beta分布
independent gamma $X,Y$, $U=X+Y$, $V=X/(X+Y)$, is gamma$(\alpha+\beta,\lambda)$ and beta$(\alpha,\beta)$.\\
%simple corollary.
independent and identical exponential rvs $X_i$, $\sum X_i$ is gamma rv with $(n,\lambda)$.\\
%rvs可交换，联合分布不变对all permutation
exchangeable, rvs' permutation no matter probability. An interesting example, urn with $n$ reds and $m$ blues, withdraw and replace along with addtional same color, $X_i$ are exchangable, i.e. $p_i= n/(n+m)$ although there are $n+m+i$ balls in that turn nonintuitively.\\
order uniform $X_i$ then $Y_i=X_{(i)} - X_{(i-1)}$ are exchangable.

\section{Properties of Expectation}
$E[\sum^n X_i] = \sum^n E[X_i]$;\\
$E[\sum^\infty X_i] = \sum^\infty E[X_i]$, not necessarily true, but holds when $X_i$ nonnegative and $\sum^\infty E[|X_i|]<\infty$.\\
The maximum-minimums identity, $\max x_i = \sum x_i - \sum_{i<j} \min(x_i,x_j) +\sum_{i<j<k}\min(.)\dots$.\\
Calculate moments, $E[{X\choose k}] =\sum_{i_1<\dots<i_k} P(A_{i_1},...,A_{i_k})$.\\
prop, independent $X,Y$, for any $g,h$, that $E[g(X)h(Y)]=E[g(X)]E[h(Y)]$.\\
def, covariance, $\cov (X,Y) =E[(X-EX)(Y-EY)]$.\\
props,
\begin{align}
	&\cov(\sum X_i, \sum Y_j) = \sum_i\sum_j \cov(X_i,Y_j) \\
	&\var(\sum X_i) = \sum \var(X_i) + 2\sum_{i<j} \cov(X_i,X_j)
\end{align}
def, correlation,
% cos t, of (X-EX), (Y-EY)
\begin{align}
	\rho(X,Y) = \frac{\cov(X,Y)}{\sqrt{\var{X}\var{Y}}}
\end{align}
conditional expectation,
\begin{align}
	& E[\sum^n X_i | Y=y] = \sum^n E[X_i|Y=y] \\
	& E[X] = E[E[X|Y]] \\
	& E[\sum^N X_i] = E[N]E[X],\text{independent $X_i,N$.}\\
	& \var(X) = E[\var(X|Y)] + \var(E[X|Y])\\
	& \var[\sum^N X_i] = E[N]\var[X] + (E[X])^2 \var(N)\\
	& E[(Y-g(X))^2] \geq E[(Y-E[Y|X])^2]
\end{align}
Moment generating function, $M(t) = E[e^{tX}]$. $M^{(n)}(0)=E[X^n]$\\
independent $X,Y$, then $M_{X+Y}(t) = M_X(t)M_Y(t)$.\\
independent,indentically, $X_i$, random $N$, $Y=\sum^N X_i$, $M_Y(t) = E[(M_X(t))^N]$.\\
joint moment generating function, 
\begin{align}
	M(t_1,...,t_n) = E[e^{t_1X_1+\dots+t_nX_n}]
\end{align}
Thm, (joint) moment generating function uniquely determine distribution.\\
Thm, independent iff $M(t_1,...,t_n) = M_{X_1}(t_1)\dots M_{X_n}(t_n)$.\\

\section{Limit Theorems}
% bound with mean
Markov's inequality, nonnegative X, $a>0$: $P(X\geq a) \leq \frac{E[X]}{a}$.\\
% bound with mean, variance
corollary, Chebyshev's inequality, X,$\mu,\sigma^2$, $k>0$, then  $P(|X-\mu|\geq k)\leq \frac{\sigma^2}{k^2}$.\\
% lim(P),    >epsilon with ->0 probability, 
Thm, The weak law of large numbers, iid(independent, identically distributed rvs) $X_i$, each finite mean $\mu=E[X_i]$, $\forall \epsilon>0$,
\begin{align}
	P(|\frac{\sum^n X_i}{n} - \mu|\geq \epsilon) \rightarrow 0, \text{as} n\rightarrow \infty
\end{align}
The central limit theorem, iid $X_i$, each with $\mu,\sigma^2$, then the distribution of 
\begin{align}
	\frac{\sum^n X_i - n\mu}{\sigma \sqrt{n}}
\end{align}
tends to standard normal distribution as $n\rightarrow \infty$. i.e. as $n\rightarrow \infty$, that
\begin{align}
	P\left(\frac{\sum^n X_i - n\mu}{\sigma \sqrt{n}} \leq a\right) \rightarrow 
	\frac{1}{\sqrt{2\pi}}\int_{-\infty}^a e^{-x^2/2}dx
\end{align}
remark. Although here show point-wise convergence, but which can unifomly convergence.\\
Central limit theorem for independent random variables, $\mu_i,\sigma_i^2$, if $X_i$ uniformly bounded, i.e. for some $M$,$P(|X_i|<M)=1,\forall i$, and $\sum \sigma_i^2=\infty$, then as $n\rightarrow \infty$
\begin{align}
	P\left(\frac{\sum^n (X_i-\mu_i)}{\sqrt{\sum^n \sigma_i^2}}\leq a\right)\rightarrow \Phi(a)
\end{align}
% P(lim),	>epsilon with =0 probability.
Thm, The strong law of large numbers, iid $X_i$, finite $\mu$. Then with probability 1,
\begin{align}
	\frac{\sum^n X_i}{n} \rightarrow \mu,\text{as }n\rightarrow \infty
\end{align}
One-sided Chebyshev inequality, $0,\sigma^2$, $a>0$, then $P(X\geq a)\leq \frac{\sigma^2}{\sigma^2+a^2}$.\\
note $X-\mu$, $\mu-X$ is $0,\sigma^2$, thus
corollay, $P(X\geq \mu +a )\leq \frac{\sigma^2}{\sigma^2+a^2}$, $P(X\leq \mu -a)\leq \frac{\sigma^2}{\sigma^2+a^2}$.\\
% bound with MGF
chernoff bounds, 
\begin{align}
	&P(X\geq a) \leq e^{-ta} M(t), \forall t>0 \\
	&P(X\leq a) \leq e^{-ta} M(t), \forall t<0 
\end{align}
% bound with expectation
Jensen's inequality, convex $f$, $E[f(X)]\geq f(E[X])$.

\end{document}