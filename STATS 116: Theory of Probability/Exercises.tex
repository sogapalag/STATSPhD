%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Short Sectioned Assignment
% LaTeX Template
% Version 1.0 (5/5/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{algpseudocode} %algorithm
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps
\usepackage{url}
\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead{} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text


\title{Exercises}
%A First Course in Probability, Ross. 8th edition
\author{sogapalag}

\date{\normalsize\today}

\begin{document}

\maketitle
\begin{itemize}
	\item[1.17] ${10 \choose 7} 7! = 604800$. Or $10*9*\dots*4 = 604800$.
	\item[1.20] 
	\begin{itemize}
		\item[(a)] ${8\choose 5} - {6\choose 3} =36= 2{6\choose 4}+{6\choose 5}$; 
		\item[(b)] ${8\choose 5} - 2*{6\choose 4} =26= {6\choose 3} + {6\choose 5}$.
	\end{itemize}
		%$4^8= 65536$; $4^8-4*3^8+12*2^8+4 = 42368$. thought different blackboard.
	\item[1.31] ${8+4-1\choose 4-1} = 165$; ${8-1\choose 4-1}=35$.
	\item[1.32] ${8+6-1 \choose 6-1} = 1287$; ${5+6-1 \choose 6-1}*{3+6-1 \choose 6-1} = 14112$.
	\item[1.33] ${9+4-1 \choose 4-1} = 220$; ${20-7+3-1 \choose 3-1} +2{13\choose 2} +{14\choose 2} +220 = 572$.
	\item[T1.5] $\sum_{i=k}^n {n\choose i}$.
	\item[T1.11] when last in $i$, sum of choices.
	\item[T1.12]
	\begin{itemize}
		\item[(a)] $x_j$ register, for each chosen which, $+1$, then sum is left; by looking each register, which be chosen $2^{n-1}$, since there are $n$ registers, sum is right. another way consider $(1_a+1_b)^n$.
		\item[(b)]
		\begin{align}
			\sum k (k-1) {n\choose k } &= n(n-1) \sum {n-2 \choose k-2}\\
			&= n(n-1)2^{n-2}
		\end{align}
		then plus $\sum k{n\choose k}$.\qed
		\item[(c)] calculate $\sum k(k-1)(k-2){n\choose k}$.\qed
	\end{itemize}
	\item[T1.13] consider $(1+(-1))^n=0$.
	\item[T1.15]
	\begin{itemize}
		\item[(a)] when maximum $x_k = j$.\qed
		\item[(b)] 35.
	\end{itemize}
	\item[T1.16]
	\begin{itemize}
		\item[(b)] when last group with $i$ players tie for.\qed
		\item[(c)] let $j=n-i$.\qed
	\end{itemize}
	\item[T1.20] $\sum x_j=n$ with $x_j\geq m_j$, let $y_j = x_j-m_j$, then equiv to $\sum y_j = n-\sum m_j$ with $y_j\geq 0$, so answer is ${n-\sum m_j + r -1 \choose r-1}$.
	\item[T1.21] consider choosing $k$ of $x_i$ set to zero, then left $x_j>0$.\qed 
	\item[T1.22] for $\frac{\partial^r}{\partial x_{i_1}\dots \partial x_{i_r}}$, different $i_j$ represent different PDE, thus total $r^n$.
	\item[T1.23]
	\begin{align}
		\sum_{i=0}^k {i+n-1 \choose n-1} &= 1+\sum_{i=1}^k \left[{i+n \choose n} - {i+n-1 \choose n}\right]\\
		&= {k+n \choose n}
	\end{align}
	\item[S1.14] recall T1.23, ${k\choose n}$. Another solution, consider $\{1,2,...,k\}$, select $n$ items, which one-to-one a vector which $x_i = y_i-y_{i-1}$.
	\item[S1.17] both in $k$ or $n-k$, or one each.\qed
	\item[2.15]
	\begin{itemize}
		\item[(a)] $4*{13 \choose 5} =  5148$; $0.00198$
		\item[(b)] $13*{4\choose 2}*{12\choose 3}*4^3 = 1098240$; $0.42257$
		\item[(c)] ${13\choose 2}{4\choose 2}^2*11*4 = 123552$; $0.04754$
		\item[(d)] $13*{4\choose 3}{12\choose 2}*4^2 = 54912$; $0.02113$
		\item[(e)] $13*12*4=624$. $0.00024$
	\end{itemize}
	\item[2.17] $8!=40320$.
	\item[2.45] 
	\begin{itemize}
		\item[(a)] shuffle the keys, solution in $k$th place, $P=1/n$.
		\item[(b)] $\frac{(n-1)^{k-1}}{n^k}$.
	\end{itemize}
	\item[2.46] consider general case, $n$ distinct items distribute to $k$ distinct groups, i.e. $\sum_{i=1}^k x_i= n$ with $x_i\geq 0$, note $k^n$ choices. Then we want find $P(\exists x\geq 2)$, which complement is $x_j\leq 1,\forall j$, with ${k \choose n} n!$ choices. Thus we get
	\begin{align}
	P(\exists x\geq 2) &= 1 - \frac{ {k \choose n}n! } { k^n } \\
		&=1 - \frac{k!}{k^n (k-n)!} \\
		&=1 - \frac{k(k-1)\dots (k-n+1)}{k^n}
	\end{align}
	let $k=12$, $P\geq 1/2$, conclude $n\geq 5$. if $k=365$, which called birthday problem, $n\geq 23$.
	\item[2.51] $\frac{ {n\choose m} (N-1)^{n-m}}{N^n}$.
	\item[2.53] one solution calculate $P(\bigcup E) = 23/35$, then answer $12/35$. Another solution, by the symmetry between couples and symmetry between husband and wife, we can consider $h_1<h_2<h_3<h_4$ and $h_i<w_i, \forall i$, then $h_1=1$, there is $7$ choices for $w_1$, after which, $h_2$ will be the minimal unseated, then $w_2$ has $5$ choices, after which $h_3$ seated, $w_3$ has $3$ choices. Thus there is $7*5*3=105$ arrangements. Now consider which arrangement is legal, for $h_1h_2h_3h_4$, left $3*3!=18$ choices; $h_1h_2h_3w_1$, note $h_4<7$, then there is $2*2!+2=6$ choices; since $w_2$ like as $w_1$, so $h_1h_2h_3$ has $18+2*6=30$ total; for $h_1h_2w_1h_3$, $2*2! + 1=5$ choices; for $h_1h_2w_1w_2h_3h_4$, only $1$ choice with connection $w_3w_4$. Then legal choices total $30+5+1=36$, $P=36/105 = 12/35$.
	\item[2.54] let $E_i$ be void in suit $i$,
	\begin{align}
		P(\bigcup E_i) =& \sum_{r=1}^4 (-1)^{r+1} \sum_{i_1<\dots<i_r} P(E_1\dots E_r) \\
			=& \frac{1}{{52\choose 13}} \left[ {4\choose 1}{39\choose 13} -6 {26\choose 13} 
			 + 4 {13 \choose 13} \right] \\
			 =& \frac{4(39*38*\dots*27) - 6(26*25*\dots*14) + 4(13!)} { 52*51*\dots*40 }\\
			 \approx & 0.051
	\end{align}
	\item[2.56] consider a vs b, $P(\text{a win}| a,b) = 5/9$; b vs c $P(\text{b win}| b,c) = 5/9$; consider a vs c, $P(\text{c win}|a,c)= 5/9$. then it's like Rock-paper-scissors game. Then B player is better.
	\item[T2.8] select $i$ items from $\{1,...,n\}$ to join $\{n+1\}$, left do partition, let $T_0=1$ thus
	\begin{align}
		T_{n+1} &= \sum_{i=0}^n {n\choose i} T_{n-i} \\
			&= 1 + \sum_{k=1}^n {n\choose k} T_k
	\end{align}\qed
	\item[T2.10] note $P(FG) = P((E\cup E^c)FG)= P(EFG\cup E^cFG)  = P(EFG)+P(E^cFG)$.\qed
	\item[T2.16] 
	\begin{align}
		& P(E_1E_2\dots E_n) \geq P(E_1)+\dots+P(E_n) -(n-1)\\
		\Leftrightarrow & P(\bigcap E_i)-1 \geq \sum [P(E_i)-1] \\
		\Leftrightarrow & P(\bigcup E_i^c) \leq \sum P(E_i^c)
	\end{align}\qed
	\item[T2.17] suppose the Nth man take the $i\in\{1,...,N-1\}$ hat, then $i$th man treat $N$th hat as his own, then $A_(N-1)$; since $N$th hat is just virtual hat his own, which can take by him indeed, then $A_(N-2)$.\
	\item[T2.18] when last is $T$, there is $f_{n-1}$ ways; when last is $H$, $n-1$ must be $T$, then $f_{n-2}$ ways.\qed
	\item[T2.19]
	\begin{align}
		P(k) &= \frac{ {n\choose 1}{n-1 \choose r-1}{m \choose k-r} (k-1)! }{ {n+m\choose k} k!} \\
			&= \frac{ r{n\choose r}{m\choose k-r}}{k{n+m\choose k }}
	\end{align}
	\item[S2.11] $4{13\choose 2} 13^3 / {52 \choose 5} = 0.26375$.
	\item[S2.20] satisfy iff the last is blue , i.e. $P = 10/(20+10) = 1/3$.
	\item[3.4] $1/6 + 5/6 * 1/6 = 11/36$.
	\item[3.13]
	\begin{align}
		P(E_1E_2E_2E_4)  &= \frac{4 {48\choose 12}}{{52\choose 13}} \frac{3 {36\choose 12}}{{39\choose 13}} \frac{2 {24\choose 12}}{{26\choose 13}} \\
		&= \frac{13^4}{{52\choose 4} } \\
		&= 0.1055
	\end{align}
	observing the result, find which is exactly the probability select 4 cards which happen to be different suits. Actually there is another way to show why these two probabilities are same and equal to above. I will prove a generalized one.\\
	Now, consider $d$ suits with each $k$ cards, i.e. a poker with total $dk$ cards. Since cards are randomly distributed, equiv to after random permutation, each one take first, second, third and fourth ... $k$ cards, i.e. one man $x_i, \forall i\in\{0,1,...,d-1\}$ take $c_{ki+1},c_{ki+2},...,c_{ki+k}$. So each one got exactly one Ace, means Aces distribute to these $d$ region, each has one, there is $k^d$ choices. And Aces total random choices is ${dk \choose d}$. So $dk$ cards into $d$ hands with $k$ cards each, the probalitity of each got Ace is:
	\begin{align}
		P(\text{Ace}) = \frac{k^d}{{dk \choose d}}
	\end{align}
	Then consider when select $d$ cards from deck, since randomly selecting, equiv to a deck in 'good' order, i.e. $c_{ki+1},c_{ki+2},...,c_{ki+k}$ are same suit, $\forall i\in\{0,1,...,d-1\}$, randomly select $d$ cards. Then it's a isomorphic situation to above, $d$ cards with different suit means that 'select' are in different region. So the probality of randomly selected $d$ cards are different suit is same as $P(\text{Ace})$.\qed
	\item[3.15]
	let $E$ be ectopic, $S$ be somoker, then
	\begin{align}
		P(S|E) &= \frac{ P(E|S)P(S) }{P(E|S)P(S)+ P(E|S^c)P(S^c)}\\
		&= \frac{2p}{2p + (1-p)} \\
		&= 0.4848
	\end{align}
	\item[3.22]
	\begin{itemize}
		\item[(a)] $5/6 * 4/6 = 5/9$
		\item[(b)] $1/3!=1/6$
		\item[(c)] $5/54$
	\end{itemize}
	\item[3.24]
	\begin{itemize}
		\item[(a)] $P(G=2|G\geq 1) = \frac{P(G=2)}{P(G\geq 1)} = \frac{1/4}{3/4} = 1/3$
		\item[(b)] $P(\text{b is gold}|\text{a is gold}) = \frac{P(\text{a,b are gold})}{ P (\text{a is gold})} = \frac{1/4}{1/2} = 1/2$. in (a) information is $G\geq 1$, in (b) information is specific one is gold.
	\end{itemize}
	\item[3.41] $p= 1/27 + 6/27 *3/51$.
	\item[3.44] A famous problem called Mont hall problem; $P(A|B^c) = P(AB^c)/P(B^c) = \frac{1/3}{2/3} = 1/2 = P(A|C^c)$. This is the jailer's reasoning, which is WRONG, since this probability based on a initial state with zero information. However in this problem or Mont hall problem, initial state is that, we choose A, information reveal is limit between B and C, let $B_*^c$ be told B free under this scenario, by symmetry $P(B_*^c)=P(C_*^c)= 1/2$, another way to think is that, $(1,0,0)$ there is $1/6$ to choose $B$, $1/3$ at $(0,0,1)$, thus $1/6+1/3=1/2$; so 
	\begin{align}
	P(A|B_*^c) &= \frac{P(AB_*^c)}{P(B_*^c)} \\
		&= \frac{P(B_*^c|A)P(A)}{P(B_*^c)} \\
		&= \frac{1/2 * 1/3} {1/2} \\
		&= 1/3
	\end{align}
	another easy way to think, suppose jailer mask B or C, we don't know, tell us(A) this masked man free, $P(A|X_*^c)=1/3$, it's easy to understand since we already know someone is free. So another man with $2/3$ excuted, A remains $1/3$. Then jailer unmask this man, since we know this man is free, this unmask just give the information about B or C each $1/2$.
	\item[3.46] intuitively, a claim will tell the conditional probability choosing male is larger or lower than the fraction, since $p_f\neq p_m$; then modify fraction will increase prediction or probability. Formal proof:
	\begin{align}
		P(A_2|A_1) &= \frac{P(A_1A_2)} {P(A_1)} \\
			&= \frac{\alpha p_m^2 + (1-\alpha)p_f^2} {\alpha p_m + (1-\alpha)p_f} \\
			&= \frac{\alpha p_m}{\alpha p_m + (1-\alpha)p_f} p_m  + \frac{(1-\alpha)p_f}{\alpha p_m + (1-\alpha)p_f}p_f \\
			&= \alpha'p_m + (1-\alpha')p_f
	\end{align}
	so $p_m>p_f$ implies $\alpha'>\alpha$, $<$ implies $<$, thus $P(A_2|A_1)>P(A_1)$.\qed
	\item[3.53] $F=\bigcup W_i$, then
	\begin{align}
		P(W_1| F) &= \frac{P(W_1F)}{P(F)} \\
			&= \frac{1/2}{1-(1/2)^n} \\
			&= \frac{2^{n-1}}{2^n -1}
	\end{align}
	\item[3.56] 
	\begin{align}
		P(new) &= \sum P(new|i)P(i) \\
			&= \sum (1-p_i)^n p_i
	\end{align}
	\item[3.57]
	\begin{itemize}
		\item[(a)] $2p(1-p)$;
		\item[(b)] $3p^2(1-p)$;
		\item[(c)] $2/3$.
	\end{itemize}
	\item[3.66]
	\begin{itemize}
		\item[(a)] $P = [1-(1-p_1p_2)(1-p_3p_4)]p_5$;
		\item[(b)] $P = p_3[1-(1-p_1)(1-p_2)][1-(1-p_4)(1-p_5)] + (1-p_3)[1-(1-p_1p_4)(1-p_2p_5)]$
	\end{itemize}
	\item[3.80]
	\begin{itemize}
		\item[(a)] $P(A_i)=1/2^i$ for $i<n$; $P(A_n)=1/2^{n-1}$.
		\item[(b)] A plays $i$ contests, which he met $i$ players, so
		\begin{align}
			P(E) &= \sum_{i=1}^n P(E|A_i)P(A_i) \\
				&= \sum_{i=1}^nP(A_i) - \sum_{i=1}^{n-1} \frac{i}{2^n-1} \frac{1}{2^i}-\frac{n}{2^n-1}\frac{1}{2^{n-1}} \\
				&= 1-\frac{x}{2^n-1} \frac{1-nx^{n-1}+(n-1)x^n}{x^2} -\frac{n}{2^n-1}\frac{1}{2^{n-1}} \\
				&= 1-\frac{1}{2^n-1} \frac{1-(n+1)x^n}{x} -\frac{n}{2^n-1}\frac{1}{2^{n-1}}\\
				&= 1-\frac{1}{2^n-1} \frac{2^n-n-1}{2^{n-1}} -\frac{n}{2^n-1}\frac{1}{2^{n-1}} \\
				&= 1- \frac{1}{2^{n-1}}
		\end{align}
		\item[(c)] I don't get it, when $n\rightarrow\infty$, given formula $P_n\rightarrow \frac{1}{4} P_{n-1}$, i.e. $P_n\rightarrow 0$, which is obviously wrong, in first round A and B has a high probability do not met.
		\item[(d)] total $2^n-1$ lose.
		\item[(e)] for every contest, equally to every one, so
		\begin{align}
			P(B_i) = \frac{1}{ {2^n\choose 2} },\ \forall i
		\end{align}
		\item[(f)] note $B_i$ are mutually exclusive, then
		\begin{align}
			P(E) &= 1 - \sum_i P(B_i) \\
				&= 1 - \frac{2^n -1 }{ {2^n\choose 2} }\\
				&= 1 - \frac{1}{2^{n-1}}
		\end{align}
	\end{itemize}
	\item[3.81] denote $P_i$ be probability of winning when distance to target 40 is $i$. Then
	\begin{align}
		P_i = p P_{i-1} + (1-p) P_{i+1}, 0<i<30
	\end{align}
	note $P_0=1, P_{30}=0$. Then
	\begin{align}
		P_i - P_{i+1}  = \frac{p}{1-p} (P_{i-1} - P_i), 0<i<30
	\end{align}
	let $d = 1 - P_1$ and $a = p/(1-p)$, we get
	\begin{align}
		\sum_{i=0}^{29} \left(\frac{p}{1-p}\right)^i d  = 1 \\
		\Rightarrow d = \frac{1-a}{1-a^{30}}
	\end{align}
	So
	\begin{align}
		P_{15}  &= 1 - \sum_{i=0}^{14} a^i d \\
			&= 1 - \frac{1-a^{15}}{1-a^{30}}\\
			&= 1 - \frac{1}{1+a^{15}} \\
			&= 0.9530
	\end{align}
	\item[3.82] let $P_A$ and $P_B$ be probabilities A and B can complete in each turn.
	\begin{itemize}
		\item[(a)(c)] let $a=2,3$ the number of heads in a row, then $P_A = P_1^a$, $P_B=P_2^a$. Then who win is that who first flip a head of coin $P_A$ or $P_B$, consider A wins in $i=1,2,...$th flip, i.e. $H_A$, $(T_AT_B)H_A$,...,$(T_AT_B)^{i-1}H_A$,..., let $\alpha = (1-P_A)(1-P_B)$, i.e. probability into next round. So probability that A wins is
		\begin{align}
			P &= \sum_{i=1}^\infty (\alpha)^{i-1}P_A \\
				&= \frac{P_A}{1-\alpha}
		\end{align} 
		\item[(b)(d)]
	\end{itemize}
	\item[3.84]
	\begin{itemize}
		\item[(a)] consider each wins in $i$th round, let $a = (2/3)^3=8/27$
		\begin{align}
			P_A &= (1+a+a^2+\dots) \frac{1}{3}\\
			&= \frac{9}{19} \\
			P_B &= (1+a+a^2+\dots) \frac{2}{3}\frac{1}{3} \\
				&= \frac{6}{19} \\
			P_C &= (1+a+a^2+\dots) \frac{2}{3}\frac{2}{3}\frac{1}{3} \\
				&= \frac{4}{19}
		\end{align}
		\item[(b)] consider white balls distributed into $12$ position, first ball in each position probability,
		\begin{align}
			P_A &= \frac{ {11\choose 3} + {8\choose 3} + {5\choose 3} }{{12\choose 4}} \\
				&= \frac{231}{495}\\
				&= 0.46667\\
			P_B &= \frac{ {10\choose 3} + {7\choose 3} + {4\choose 3} }{{12\choose 4}}\\
				&= \frac{159}{495}\\
				&= 0.32121\\
			P_C &= \frac{ {9\choose 3} + {6\choose 3} + {3\choose 3} }{{12\choose 4}}\\
				&= \frac{105}{495}\\
				&= 0.21212
		\end{align}
	\end{itemize}
	\item[3.86] For any element $j$, either in A or not, either in B or not, only when in A and not in A, A is not subset of B, so there are $3/4$ for element $j$ satisfy subset requirement, and each $j$ is selected independently, so
	\begin{align}
		P(A\subset B) = \left(\frac{3}{4}\right)^n
	\end{align}
	For $AB=\emptyset$ when all $j$ not both in A and B would satisfy, so also
	\begin{align}
		P(AB=\emptyset) = \left(\frac{3}{4}\right)^n
	\end{align}
	\item[T3.14] denote $P_i$ the probability eventually go broke when initial fortune is $i$, note $P_i =pP_{i+1}+qP_{i-1}, i>0$, $P_0=1$. thus
	\begin{align}
		q(P_i-P_{i-1}) = p(P_{i+1}-P_i)
	\end{align}
	if $p\leq\frac{1}{2}$ and $P_1<1$, then $P_{i+1}-P_i \geq P_i-P_{i-1}$, that $\exists N$, s.t. $P_n<0$ when $n>N$, which is contradicted to $P_i>0$, so $P_1=1$, thus $P_i=1$.\\
	when $p>\frac{1}{2}$, let $P_1=1-d$, then
	\begin{align}
		P_i = 1 - d\frac{1-(q/p)^i}{1-q/p}
	\end{align}
	note $i\rightarrow\infty$, $P_i\rightarrow 0$, so $d= 1-q/p$, thus
	\begin{align}
		P_i = (q/p)^i
	\end{align}\qed
	\item[T3.23] Induction, suppose $P_{a,b}=\frac{1}{2}$, while $a>0, b>0$. where $P_{1,1}=\frac{1}{2}$ is trivial, Now consider start with $a,b$, by the time procedure back to step 1, either still satisfy $a'>0,b'>0$, or discard all white balls or all black balls, i.e.
	\begin{align}
		P_{a,b} = P(a'>0,b'>0)\frac{1}{2} + P(a'=0)0 + P(b'=0)1
	\end{align}
	note that $P(a'=0)=P(b'=0) = 1/{a+b \choose a}$, thus $P_{a,b} = 1/2$.\qed
	\item[T3.24] By hint, consider $B_i$, since each other player against $k$ players set is independent,
	\begin{align}
		P(B_i) = \left[ 1 - \left(\frac{1}{2}\right)^k \right]^{n-k}
	\end{align}
	By Boole's inequality and problem assumption
	\begin{align}
		P(\bigcup B_i) \leq \sum P(B_i) = {n\choose k} \left[ 1 - \left(\frac{1}{2}\right)^k \right]^{n-k}<1
	\end{align}
	that probability of outcome is
	\begin{align}
		P = 1 - P(\bigcup B_i) >0
	\end{align}
	which shows possibility.\qed
	\item[T3.29] Recall Laplace's rule of succession,
	\begin{align}
		P(C_i|F_n) = \frac{(i/k)^n[1/(k+1)]}{\sum_{j=0}^k(j/k)^n[1/(k+1)]}
	\end{align}
	next $m$ flips all heads under selected $C_i$ is 
	\begin{align}
		P(H^m|F_nC_i) = P(H^m|C_i) = (i/k)^m
	\end{align}
	the probability that flips all heads is
	\begin{align}
		P(H^m|F_n) = \frac{\sum_{i=0}^k (i/k)^{n+m}}{\sum_{j=0}^k(j/k)^n}
	\end{align}
	when $k$ is large,
	\begin{align}
		\frac{1}{k}\sum_{i=0}^k (i/k)^{n+m} &\approx  \int_0^1 x^{n+m}dx = \frac{1}{n+m+1}\\
		\frac{1}{k}\sum_{j=0}^k (j/k)^{n} &\approx  \int_0^1 x^{n}dx = \frac{1}{n+1}\\
		P(H^m|F_n) &\approx \frac{n+1}{n+m+1}
	\end{align}\qed
	\item[T3.30] denote $F_{n,r}$ that first $n$ flips results $r$ heads.
	\begin{align}
		P(C_i|F_{n,r}) &= \frac{{n\choose r}(i/k)^r(1-i/k)^{n-r} 1/(k+1)}{\sum_{j=0}^k{n\choose r}(j/k)^r(1-j/k)^{n-r}[1/(k+1)]} \\
			&= \frac{(i/k)^r(1-i/k)^{n-r} }{\sum_{j=0}^k(j/k)^r(1-j/k)^{n-r}}
	\end{align}
	\begin{align}
		P(H|F_{n,r} C_i) = P(H|C_i) = i/k
	\end{align}
	\begin{align}
		P(H|F_{n,r}) = \frac{\sum_{i=0}^k(i/k)^{r+1}(1-i/k)^{n-r} }{\sum_{j=0}^k(j/k)^r(1-j/k)^{n-r}}
	\end{align}
	when $k$ is large,
	\begin{align}
		\frac{1}{k}\sum_{i=0}^k(i/k)^{r+1}(1-i/k)^{n-r} &\approx \int_0^1 y^{r+1}(1-y)^{n-r}dy = \frac{(r+1)!(n-r)!}{(n+2)!}\\
		\frac{1}{k}\sum_{j=0}^k(j/k)^{r}(1-j/k)^{n-r} &\approx \int_0^1 y^{r}(1-y)^{n-r}dy = \frac{r!(n-r)!}{(n+1)!}\\
		P(H|F_{n,r}) &\approx \frac{r+1}{n+2}
	\end{align}\qed
	\item[S3.7] denote $AA$ be event double Aces, $A_\spadesuit$ be ace of spade.
	\begin{itemize}
		\item[(a)]
		\begin{align}
			P(AA|\exists A_\spadesuit) &= \frac{\frac{1}{2} \frac{4*3}{52*51}}{\frac{1}{2} \frac{4*3}{52*51} + \frac{2}{52}\frac{48}{51}}\\
				&= \frac{1}{17}
		\end{align}
		\item[(b)] trivial guess $3/51$, formal proof is
		\begin{align}
			P(AA|C_1=A) &= \frac{\frac{4*3}{52*51}}{\frac{4*3}{52*51} + \frac{4}{52}\frac{48}{51}}\\
					&= \frac{3}{51}
		\end{align}
		\item[(c)] select order just relable, still $3/51$, formal proof is
		\begin{align}
			P(AA|C_2=A) &= \frac{\frac{4*3}{52*51}}{\frac{4*3}{52*51} + \frac{48}{52}\frac{4}{51}}\\
					&= \frac{3}{51}
		\end{align}
		\item[(d)] 
		\begin{align}
			P(AA|\exists A) &= \frac{\frac{4*3}{52*51}}{ \frac{4*3}{52*51} + \frac{4}{52}\frac{48}{51} + \frac{48}{52}\frac{4}{51} } \\
				&=\frac{1}{33}
		\end{align}
	\end{itemize}
	\item[S3.13] 
	\begin{itemize}
		\item[(a)(b)] Consider last of reds and blues, additional green doesn't matter, i.e. still $1/3$.
		\item[(c)] $P(b<r<g)=\frac{20}{20+10}\frac{8}{38} = 0.0702$
		\item[(d)] $P(b) = P(b<r<g) + P(b<g<r) = 0.0702 + 8/18 *20/38=0.3041$
	\end{itemize}
	\item[S3.20] note symmtry $P(i<j) = P(i>j)$,(or just simplify $\sum_{i<j}p_ip_j$) thus
	\begin{align}
		P(i<j)=\frac{1 -\sum p_i^2}{2}
	\end{align}
	\item[S3.21] let $p_=$ be probability heads equal when both $n$ flips of A and B, then by symmetry $p_>= p_< = \frac{1-p_=}{2}$, thus probability of $H_{A,n+1}>H_{B,n}$ while $n+1,n$ coins fliped by A,B is
	\begin{align}
		P(H_{A,n+1}>H_{B,n}) &= \frac{1}{2}p_= + p_> \\
			&= \frac{1}{2}
	\end{align}\qed
	\item[S3.26] by $P(A|B)= 1$, note $P(A^c|B)=0$ thus
	\begin{align}
		P(B^c|A^c) &= \frac{P(A^c|B^c)P(B^c)} {P(A^c|B^c)P(B^c) + P(A^c|B)P(B)}\\
			&= 1
	\end{align}\qed
	\item[S3.27] $k=1$, $1$ or $2$ reds each $1/2$ clearly. Suppose $k=n$ is satisfied, when $k=n+1$, the probability of exactly $i$ reds is
	\begin{align}
		P(i|n+1) &= P(i|n)\frac{n+2 - i}{n+2} + P(i-1|n)\frac{i-1}{n+2}\\
			&=\frac{1}{n+1} \frac{n+1}{n+2}\\
			&=\frac{1}{n+2}
	\end{align}\qed
	\item[3.28] denote $A_i$ event player $i$ receives ace,
	\begin{align}
		P(A_i) &= 1-\frac{{n\choose 2}}{{2n\choose 2}}\\
			&= 1 - \frac{n-1}{2(2n-1)}\\
			&= \frac{3n-1}{4n-2}
	\end{align}
	and both receive ace, one way to think left ace to choose, or another way to think 2 aces' distribution, i.e.
	\begin{align}
		P(A_1A_2) = \frac{n}{2n-1} = \frac{n^2}{{2n\choose 2}}
	\end{align}
	then
	\begin{align}
		P(A_2^c|A_1) &= 1-P(A_2|A_1) \\
			&= 1 - \frac{P(A_1A_2)}{P(A_1)}\\
			&= \frac{n-1}{3n-1}
	\end{align}
	when $n\rightarrow\infty$, $P(A_2^c|A_1)\rightarrow 1/3$; since Aces becomes 'independent', $1/2$ for each, i.e. given one of coins is H, prob of another also H is $1/3$.
	\item[S3.29]
	\begin{itemize}
		\item[(a)] $P=n!\prod p_i$;
		\item[(b)] note $1-P = P(\bigcup E_i)$, where
		\begin{align}
			P(\bigcup E_i) &= \sum_{r=1}^n (-1)^{r+1} \sum_{i_1<\dots<i_r} P(E_1\dots E_r)\\
				&= \sum_{r=1}^n (-1)^{r+1} {n\choose r} \left(\frac{n-r}{n}\right)^n
		\end{align}
		thus 
		\begin{align}
			\frac{n!}{n^n} =  1 - P(\bigcup E_i) = \sum_{r=0}^n (-1)^{r} {n\choose r} \left(\frac{n-r}{n}\right)^n
		\end{align}\qed
	\end{itemize}
	\item[S3.30]
	\begin{align}
		P(E|E\cup F) &= P(E|(E\cup F)F) P(F|E\cup F) + P(E|(E\cup F)F^c)P(F^c|E\cup F)\\
			&= P(E|F) P(F|E\cup F) + P(E|E) P(F^c|E\cup F)\\
			&= P(E|F) k + (1-k)\\
			&\geq P(E|F)
	\end{align}\qed
	\item[4.24] 
	\begin{itemize}
		\item[(a)] $E[X_B|A=1] = p -(1-p)\frac{3}{4} =\frac{7p-3}{4}$;
		\item[(b)] $E[X_B|A=2] = -\frac{3}{4}p + 2(1-p) = \frac{8-11p}{4}$;
		\item[(c)] $-E[X_A|B=1] = -q + (1-q)\frac{3}{4} = \frac{3-7q}{4}$;
		\item[(d)] $-E[X_A|B=2] =  q\frac{3}{4} - 2(1-q) = \frac{11q-8}{4}$.
	\end{itemize}
	note $7q-3\geq 8-11q$ when $q\geq 11/18$, so when $q=11/18$, that $E[X_A] = 23/72$ is minmax loss. $7p-3\leq 8-11p$ when $p\leq 11/18$, so when $p=11/18$, that $E[X_B] = 23/72$ is maxmin gain.
	\item[4.30] recall geometry distribution, $E[X] = \sum_{n=1}^\infty 2^n(1-1/2)^{n-1}(1/2) =\sum 1 = +\infty$. Once is too risky, arbitrary long is good.
	\item[4.31] p.
	\item[4.56] $1 - (364/365)^N > 1/2$, that $N>252.6$ so need $253$.
	\item[4.62] all $p_i$ small enough, two specific trial occur double $i$ or double $j$ becomes 'independent', and since $p_i$ small enough, occur more than 3 times is $o(p_i^2)$, thus $X$ the number of occur same outcome is a Poisson random variable of $\lambda = {n\choose 2} \sum p_i^2$. So probability of no trial outcome occurs more than once is $P(X=0) = e^{-\lambda}$.\qed\\
	recall birthday problem, we can use this solution to approximate, i.e. let $k=365$, $p_i=1/365, \forall i$, then $1/2$ inequality is
	\begin{align}
		e^{-\frac{n(n-1)}{2k}} < \frac{1}{2} \\
		n > \frac{1}{2} + \sqrt{\frac{1}{4} + 2k\ln(2)}\approx 22.9999
	\end{align}
	\item[4.66]
	\begin{itemize}
		\item[(a)] arbitrary set one, another have 2 next-to seats choice, i.e. $P(C_i) = \frac{2}{2n-1}$;
		\item[(b)] when $C_i$ set, there remain a row of $2n-2$ seats, so 
		\begin{align}
			P(C_j|C_i)  = \frac{2n-3}{{2n-2\choose 2}} = \frac{2}{2n-2} =\frac{1}{n-1}
		\end{align}
		\item[(c)] when $n$ is large, $P(C_j)\approx P(C_j|C_i)\approx 1/n$, independent, that $\sum P(C_i)\approx 1$, let $X$ the number of occur seated next-to couples, then become a Poisson random variable of $\lambda=1$, then probability of no couple seated next to each other is $P(X=0)=e^{-1}$. Another solution consider $P(\bigcap C_i^c) = (1-1/n)^n\rightarrow e^{-1}$ achieve same answer.
	\end{itemize}
	\item[4.75] $Y=X+10$ is nega bino $(10,1/2)$.
	\item[4.76] let $n=N_1+N_2-k+1$, $r_1 = N_1+1$ and $r_2=N_2+1$. Then left-empty becomes $n,r_1$ nega bino and right-empty becomes $n,r_2$ nega bino; the probability is
	\begin{align}
		P(k) &= {n-1\choose r_1-1}(1/2)^n + {n-1\choose r_2-1}(1/2)^n \\
			&= \left[{N_1+N_2-k \choose N_1}+{N_1+N_2-k \choose N_2}\right] (1/2)^{N_1+N_2-k+1}
	\end{align}
	\item[4.79] recall hypergenometry random variable, $m=6,N=100,n=10$, thus
	\begin{align}
		P(X=0) &= \frac{{6\choose 0}{94\choose 10}}{{100\choose 10}} = 0.5223 \\
		P(X=1) &= \frac{{6\choose 1}{94\choose 9}}{{100\choose 10}} = 0.3687 \\
		P(X=2) &= \frac{{6\choose 2}{94\choose 8}}{{100\choose 10}} = 0.0965 \\
		P(X>2) &= 1-P(X=0)-P(X=1)-P(X=2) = 0.0125
	\end{align}
	\item[4.80]
	\begin{itemize}
		\item[(a)] $p=20*19/(80*79)$, so fair payoff $v=(1-p)/p = 15.63$;
		\item[(b)] recall hypergenometry random variable, i.e.
		\begin{align}
			P_{n,k} = \frac{{20\choose k}{60\choose n-k}}{{80\choose n}}
		\end{align}
	\end{itemize}
	\item[4.81]
	\begin{itemize}
		\item[(a)] define $E_i$ indicator occur empty for box $i$, i.e.
		\begin{align}
			E_i = \begin{cases} 1, &\mbox{if box $i$ empty} \\ 0, &\mbox{if not empty}\end{cases}
		\end{align}
		Then the expected number of empty boxes is
		\begin{align}
			E[\sum E_i] = \sum E[E_i] =\sum (1-p_i)^{10}
		\end{align}
		define $A_i$ the exactly one ball indicator, then expected number is
		\begin{align}
			E[\sum A_i] = \sum E[A_i] =\sum 10(1-p_i)^9p_i
		\end{align}
	\end{itemize}
	\item[4.85] define $C_i$ the indicator for type $i$ coupon appear at least once, i.e.
	\begin{align}
		C_i = \begin{cases} 1, &\mbox{if type $i$ coupon appear} \\ 0, &\mbox{if not}\end{cases}
	\end{align}
	then expected number of distinct types appear is
	\begin{align}
		E[\sum C_i] &= \sum E[C_i] \\
			&= \sum (1 - (1-p_i)^n) \\
			&= k - \sum (1-p_i)^n
	\end{align}
	\item[T4.13] consider $d/dp$, then $p=k/n$.
	\item[T4.14]
	\begin{itemize}
		\item[(a)] $1 - \alpha p/(1-p)$;
		\item[(b)] let $P_k$ be probability of $k$ boys. Then
		\begin{align}
			P_k &= \sum_{n=k}^\infty \alpha p^n {n\choose k} (1/2)^{k}(1/2)^{n-k}\\
				&= \alpha \sum_{n=k}^\infty{n\choose k}(p/2)^n \\
				&= \alpha \sum_{n=k}^\infty [{n+1\choose k+1} -{n\choose k+1}](p/2)^n \\
				&= \frac{2}{p} P_{k+1}  -  P_{k+1}\\
				&= \frac{2-p}{p} P_{k+1}
		\end{align}
		note $P_1 = \sum_{n=1}^\infty \alpha n(p/2)^n =  \frac{\alpha p/2}{(1-p/2)^2}$, thus
		\begin{align}
			P_k = (\frac{p}{2-p})^{k-1} P_1
		\end{align}
	\end{itemize}
	\item[T4.15] $(p+q)^n+(q-p)^n$ eliminate $(-p)^{2k+1}$, thus remain even.\qed
	\item[T4.17] recall 4.15, consider $\pm\lambda$.
	\item[T4.18] by $d/d\lambda$, note $\lambda = \sqrt{k}$.
	\item[T4.22] for numbered $i$ balls, $p_i = k/{2n\choose 2}$, when $n$ large, each independent, and $\sum p_i \approx k/(2n)$, i.e. $\lambda =1/(2n)$, then $T>\alpha n$, means a Poisson with parameter $\lambda \alpha n = \alpha/2$'s probability $P(N=0) = e^{-\alpha/2}$.\qed
	\item[T4.24] recall Example7d, that we get two equivalent recursive formula.
	\begin{align}
		P_n &= P_{n-1} + (1-P_{n-k-1})(1-p)p^k \\
		P_n &= \sum_{j=1}^k P_{n-j}p^{j-1}(1-p) + p^k
	\end{align}
	First condition on either first $n-1$ occur, second condition on first head location.
	\item[S4.26]
	\begin{itemize}
		\item[(a)]
		\begin{align}
			\alpha &= \sum_{i=1}^\infty  p(1-p)^{2i-1} \\
				&= p(1-p) \sum_{i=1}^\infty (1-p)^{2(i-1)} \\
				&= \frac{p(1-p)}{1-(1-p)^2} \\
				&= \frac{1-p}{2-p}
		\end{align}
		\item[(b)] note $P(\text{even}|X=1)=0$ and $P(X>1)=1-p$, that
		\begin{align}
			(1-p)(1-\alpha) = \alpha \\
			\Rightarrow \alpha = \frac{1-p}{2-p}
		\end{align}
	\end{itemize}
	\item[T5.32]
	\begin{itemize}
		\item[(a)] when $N$ is large, X and Y randomly set into $\mathbb{N}$, thus probability which can divided by $k$, is $1/k$; under this condition, $X/k$, $Y/k$ can be treated randomly set into $\mathbb{N}$, the probability of relatively prime is $Q_1$, so we get $Q_k = \frac{1}{k^2}Q_1$.
		\item[(b)] note $\sum_{k=1}^\infty Q_k =1$, thus
		\begin{align}
			Q_1 = \frac{1}{\sum_{k=1}^\infty 1/k^2} = \frac{6}{\pi^2}
		\end{align} 
		\item[(c)] recall 4.11, X and Y can divided by prime $P_i$ with probability $1/P_i$, since $N$ is large, which distinc primes' are independent, $Q_1$ means X and Y have no common $P_i$, denote $E_i$ event X and Y not both divided by $P_i$, i.e. 
		\begin{align}
			Q_1 &= P\{\bigcap_{i=1}^\infty E_i\} \\
				&= \prod_{i=1}^\infty P(E_i) \\
				&= \prod_{i=1}^\infty (1-\frac{1}{P_i^2})\\
				&= \prod_{i=1}^\infty (\frac{P_i^2-1}{P_i^2})
		\end{align}
	\end{itemize}
	Thus we in a sense proved the equality
	\begin{align}
		\prod_{i=1}^\infty \frac{1}{1-p_i^{-2}} = \sum_{n=1}^\infty \frac{1}{n^2} = \frac{\pi^2}{6} = \zeta(2)
	\end{align}
	where $\zeta(s)$ is famous Riemann Zeta function defined as
	\begin{align}
		\zeta(s) = \sum_{n=1}^\infty \frac{1}{n^s} = \frac{1}{1^s} + \frac{1}{2^s} +\frac{1}{3^s} + \dots
	\end{align}
	one way to show equility is that
	\begin{align}
		\prod_{i=1}^\infty \frac{1}{1-p_i^{-s}} = \prod_{i=1}^\infty  (1+p_i^{-s}+p_i^{-2s}+\dots)
	\end{align}
	since every $n\in \mathbb{N}$ can be factorize, there is a bijection $n^{-s}$ to $\prod_i p_i^{-k_is}$, so equility holds.\qed
	\item[6.6] select 2 position for defective ones, i.e. ${5\choose 2}$ ways equally, $1/10$ each.
	\item[6.9]
	\begin{itemize}
		\item[(a)] $\int\int f(x,y)dxdy = 1$;
		\item[(b)] $f_X(x)=\int f(x,y)dy = \frac{6}{7}(2x^2+x)$;
		\item[(c)] $P(X>Y)=\int_0^1 \int_0^x f(x,y)dydx = 15/56$;
		\item[(d)]
		\begin{align}
			P(Y>\frac{1}{2}| X<\frac{1}{2}) &= \frac{P(Y>\frac{1}{2} , X<\frac{1}{2})}{P(X<\frac{1}{2})} \\
				&= \frac{\int_{1/2}^2 \int_0^{1/2}f(x,y)dxdy} {\int_{0}^2 \int_0^{1/2}f(x,y)dxdy}\\
				&= \frac{\int_{1/2}^2 \frac{6}{7}(\frac{1}{24}+\frac{y}{16})dy}{\int_{0}^2 \frac{6}{7}(\frac{1}{24}+\frac{y}{16})dy}\\
				&= \frac{\frac{1}{24}\frac{3}{2}+\frac{1}{16}\frac{1}{2}(4-\frac{1}{4})}{\frac{1}{24}2+\frac{1}{16}\frac{1}{2}4}\\
				&= \frac{69}{80}
		\end{align}
		\item[(e)]
		\begin{align}
			E[X] = \int_0^1 \frac{6}{7}x(2x^2+x)dx = \frac{5}{7}
		\end{align}
		\item[(f)]
		\begin{align}
			E[Y] = \int\int yf(x,y)dxdy = \frac{8}{7}
		\end{align}
	\end{itemize}
	\item[6.16]
	\begin{itemize}
		\item[(a)] $A=\bigcup A_i$;
		\item[(b)] since $i,j$ can be same point, so $A_i\cap A_j\neq \emptyset$, they are not mutually exclusive; however $P(A_i\cap A_j) = 0$.
		\item[(c)] by $P(A_i\cap A_j) = 0$, we can get
		\begin{align}
			P(A) &= \sum P(A_i) \\
				&= n \frac{1}{2^{n-1}}
		\end{align}
	\end{itemize}
	\item[6.44] recall order statistics, that
	\begin{align}
		P(X_{(1)}+X_{(2)} < X_{(3)}) &= 3!\int_0^{1/2} \int_{x_1}^{1-x_1}\int_{x_1+x_2}^{1}dx_3 dx_2 dx_1 \\
			&= \frac{1}{2}
	\end{align}
	A incorrect way to do, consider $X_1<X_2<X_3$, under this condition, $X_2$ is uniform on $(0,X_3)$, $X_1$ is uniform on $(0,X_2)$, (we consider the complement $X_1+X_2>X_3$) thus get
	\begin{align}
		P^* &= \int_0^1 \int_{x_3/2}^{x_3} \frac{1}{x_3}\int_{x_3-x_2}^{x_2}\frac{1}{x_2} dx_1dx_2dx_3\\
			&= \int_0^1 \int_{x_3/2}^{x_3} \frac{2x_2 - x_3}{x_2 x_3} dx_2dx_3 \\
			&= \int_0^1 \int_{1/2}^{1} (2 - \frac{1}{t})dt dx_3 \ \ \  \text{let $t=x_2/x_3$}\\
			&= 1 - \ln(2)
	\end{align}
	which shows if we uniformly select $X_3$, then uniformly select $X_2$ on $(0,X_3)$, then uniformly select $X_1$ on $(0,X_2)$, the probability $X_3>X_1+X_2$ is $\ln(2)\neq 1/2$. Although these two scenarios have same conditional uniform distribution, the intrinsic difference is that the portion of distributions not same, the latter will make probability on $(0,1)$ left denser, a transform to first solution is change $dx_2$ into $dF$, where $F=x_3x_2$. which distributions' portion will make sum of distributions back to uniform.\\
	Another quick way to see, let $Y_i= X_{(i)}-X_{(i-1)}$, $Y_1=X_{(1)}$; recall exchangable, $X_{(3)}>X_{(1)}+X_{(2)}$ is indeed $Y_3>Y_1$, which by symmetry $P=1/2$.\\
	If we define $Y_{n+1}=1 -X_{(n)}$, by interval's left-right symmetry, easy to know $Y_2,...,Y_{n+1}$ are exchangable, and $n+1$ interval $Y_i$ are symmetry, actually any $n$ of $Y_i$ are exchangle, and we wonder whether $n+1$ $Y_i$ are exchangable, which is not, since the Jacob is 0, cause $n$ dimension into $n+1$ dimension, with $\sum_{i=1}^{n+1} Y_i =1$. Which probability is actually 0. Well the probability is under $n+1$ dimensional space, we can measure which on $n$ dimensional space with $n$-variables' vector. e.g. $x_1+x_2=1$ make a line.\\
	So although they are not exchangable in a $n+1$-dim sense, but since they are symmetric, either two compare order's probability will be $1/2$. Now consider $n=2$, that $X_1,X_2$ make a $Y_1,Y_2,Y_3$ intervals. We wonder (a) what $P(Y_2>Y_1, Y_2>Y_3)$ is? (b) what $P(Y_2>Y_1+Y_3)$ is?
	\begin{itemize}
		\item[(a)] By symmetry of $Y_i$, easy to get $P(Y_2>Y_1, Y_2>Y_3) = 1/3$. A calculative proof is that $0,1-2y_2 < y_1 < y_2, 1-y_2$, thus we need to integral partly,
		\begin{align}
			P(Y_2>Y_1, Y_2>Y_3) &= 2! [\int_{1/3}^{1/2}\int_{1-2y_2}^{y_2} dy_1dy_2 + \int_{1/2}^1\int_{0}^{1-y_2}dy_1dy_2] \\ 
				&= 2![\frac{3}{2}\frac{5}{36}-\frac{1}{6} + \frac{1}{8}]\\
				&= \frac{1}{3}
		\end{align}
		\item[(b)] $Y_2>Y_1+Y_3$ is indeed equivalent to $Y_2>1/2$, one might get the answer which is the probability, however this is wrong, since $Y_i$ is not uniform, either $X_{(1)}$ not, only $X_i$ is uniform. The formal proof is that,
		\begin{align}
			P(Y_2>Y_1+Y_3) &= 2! \int_{1/2}^1 \int_0^{1-y_2}dy_1dy_2 \\
				&= \frac{1}{4}\\
			\text{or equiv to consider $Y_1=X_{(1)}$}\\
			P(X_{(1)}>1/2) &= 2! \int_{1/2}^1 \int_{1/2}^{x_2} dx_1dx_2\\
				&= \frac{1}{4}
		\end{align}
		An interesting solution is that, imagine we connect point $0$ and $1$, make the interval curve to a circle. Then $Y_2>1/2$ iff $X_{(2)}- X_{(1)}$' angle $>\pi$ iff point $0$ in angle $<\pi$ of $X_1,X_2$. i.e. if we denote $x_0$ point $0$, $x_1$ correponding to $X_1$, $x_2$ corresponding to $X_2$, then $x_0$ in the small semi-circle of $x_1$ and $x_2$. Now, let $x_0$ be random too, (one might argue this $x_1-x_0$ will still be uniform? Indeed, let $x_0$ random is add a additional dimension, i.e. coordinate rotation, which not interfere $x_1-x_0$'s randomness, actually, even $x_0$ not random, like rotate some angle $\theta$, $x_1-x_0$ still random.) Note a necessary condition is $x_0,x_1,x_2$ is some semi-circle, recall 6.16, $P(A)=3/4$, since $x_0,x_1,x_2$ are symmetric, $x_0$ in the middle will be sufficient i.e. $P=3/4 * 1/3 = 1/4$.\\
		If we ask (c) what about there exists one $Y_i>1/2,i=1,2,3$?
		\item[(c)] note in semi-circle is necessary and sufficient, $P=3/4$.
	\end{itemize}
	Now consider general case $n$, (d) $P(Y_i>Y_j,\forall j\neq i, j\in\{1,2,...,n+1\})$, (e) $P(Y_i>1/2)$ and $P(\exists i, Y_i>1/2)$.
	\begin{itemize}
		\item[(d)] by symmetry, $Y_i$ is largest interval with $P=1/(n+1)$.
		\item[(e)] $P(\exists i, Y_i>1/2)= (n+1)\frac{1}{2^n}$ is easy to know, then by symmetry of $Y_i$ or symmetry of $x_0,...,x_n$ which $x_0$'s position corresponding to the large interval. So $P(Y_i>1/2) = \frac{1}{2^n}$.
	\end{itemize}
	What about (f) $P(X_{(n)} > X_{(1)} + \dots + X_{(n-1)})$?
	\begin{itemize}
		\item[(f)] note $X_{(i)} = Y_i+Y_{i-1}+\dots+Y_1$
		\begin{align}
			&X_{(n)} > X_{(1)} + \dots + X_{(n-1)} \\
			\Leftrightarrow & Y_n > Y_{n-2}+ 2Y_{n-3}+ \dots+ (n-2)Y_1 
		\end{align}
		note that
		\begin{align}
			Y_n < 1 - (Y_1 + Y_2 +\dots + Y_{n-2} +Y_{n-1}) 
		\end{align}
		thus integral part for $y_n$ is
		\begin{align}
			&\int_{y_{n-2}+ 2y_{n-3}+ \dots+ (n-2)y_1}^{1 - (y_1 + y_2 +\dots + y_{n-2} +y_{n-1})} dy_n \\
			=& 1 - (n-1)y_1 -(n-2)y_2 -\dots- 2 y_{n-2} -y_{n-1}
		\end{align}
		where $(y_1,y_2,...,y_{n-2},y_{n-1})$ exactly satisfy $1 - (n-1)y_1 -(n-2)y_2 -\dots- 2 y_{n-2}-y_{n-1}>0$
		let $t_1= (n-1)y_1, t_2 = (n-2)y_2,...,t_{n-2} = 2 y_{n-2}$, then integral becomes
		\begin{align}
			I = n! \frac{1}{(n-1)!} \int (1-t_1-t_2-\dots-t_{n-2}-y_{n-1}) dt_1dt_2\dots dt_{n-2}dy_{n-1}
		\end{align}
		with conditions
		\begin{align}
			& t_1 + t_2 + \dots + t_{n-2} <1 \\
			& 0<t_1 <n-1 \Rightarrow 0<t_1<1 \ \ \text{by last equation}\\
			& 0<t_2 <n-2 \Rightarrow 0<t_2<1\\
			&\dots\\
			& 0<t_{n-2}<2 \Rightarrow 0<t_{n-2}<1 \\
			& 0<y_{n-1}<1
		\end{align}
		note that
		\begin{align}
			\int_0^a (a-x)^kdx = \frac{1}{k+1} a^{k+1}
		\end{align}
		so integral is
		\begin{align}
			I =& n \int_0^1 \int_0^{1-y_{n-1}}\dots \int_0^{1-t_3-\dots-t_{n-2}-y_{n-1}}\int_0^{1-t_2-t_3-\dots-t_{n-2}-y_{n-1}} \\
			&(1-t_1-t_2-\dots-t_{n-2}-y_{n-1}) dt_1dt_2\dots dt_{n-2}dy_{n-1} \\
			=& n \frac{1}{n!} \\
			=& \frac{1}{(n-1)!}
		\end{align}
		so the probability is $P=I=1/(n-1)!$. Which equiv to $P(\max\{X_i\} > \sum X_i /2)$.
	\end{itemize}
	\item[6.46] Recall Example 6a, first standardize $\beta = d/L \leq 1/2$, the problem is indeed $P(Y_2>\beta,Y_3>\beta)$.
	\begin{align}
		P(Y_2>\beta,Y_3>\beta) &= 3! \int_0^{1-2\beta}\int_\beta^{1-y_1-\beta}\int_\beta^{1-y_1-y_3}dy_2dy_3dy_1\\
			&= (1-2\beta)^3
	\end{align}
	\item[6.48]
	\begin{itemize}
		\item[(a)] one solution is using DeMorgan'law, since rvs are independent, $P(X>a)=e^{-\lambda a}$, thus
		\begin{align}
			P\{\min(X_1,...,X_5)\leq a\} &= 1 - P(\bigcap X_i>a) \\
				&= 1- e^{-5\lambda a}
		\end{align}
		\item[(b)]
		\begin{align}
			P\{\max(X_1,...,X_5)\leq a\} &= P(\bigcap X_i\leq a) \\
				&= (1 - e^{-\lambda a })^5
		\end{align}
	\end{itemize}
	\item[6.52] Recall Example 7b, $J= 1/r$, thus joint density function is $r/\pi$.
	\item[T6.11]
	\begin{itemize}
		\item[(a)] by hint, rewrite integral
		\begin{align}
			I &= \int_0^1\int_{u_1}^1\int_{u_2}^1\int_{u_3}^1\int_{u_4}^1 dF_5dF_4dF_3dF_2dF_1 \\
				&= \frac{1}{5!}
		\end{align}
		\item[(c)] intuitively, the are independent and identical, thus by symmetry any order with same probability which is inverse of number of permutations.
	\end{itemize}
	\item[T6.21]
	\begin{align}
		f_{W|N}(w|n) &= \frac{e^{-w}w^n/n! * f(w)}{\int p_{N|W}(n|w)dw} \\
			&= C e^{-(\beta+1)}w^{t+n-1}
	\end{align}
	the form of gamma distribution with parameters $(t+n,\beta+1)$.\qed
	\item[T6.22] Still concentrate on non constant part,
	\begin{align}
		f(w|x_i,...) &= C f(w)\prod we^{-wx_i} \\
			&= K e^{-(\beta+\sum x_i)w} w^{t+n-1}
	\end{align}
	thus the form of gamma distribution with parameters $(t+n, \beta+\sum x_i)$.\qed
	\item[T6.26] standardize $\beta=D/L\leq 1/(n-1)$, then indeed $Y_i\geq \beta,i=2,3,...,n$. Recall Example 6a and 6.46, one can integral to the answer $(1-(n-1)\beta)^n$. From the elegant form of answer, we let $Z_{(i)} = X_{(i)} - (i-1)\beta$, then $Z_{(i)}$ in a region of $(0, 1-(n-1)\beta)$, and order equivlent, so probability is $(1-(n-1)\beta)^n$. There are many explainations, which is a bijective without scale, (i.e. scale$=1$), a translation of origin to $(0,\beta,2\beta,...,(n-1)\beta)$.\qed
	\item[T6.28] $x$ is median, means there are less and geater $x$ each $n$ numbers, since every number is uniform, thus $f(x)=Cx^n(1-x)^n$, the form of beta distribution with parameters $(n+1,n+1)$.\qed
	\item[T6.31] Recall 6.44(e), and 6.16; it's a generalized case. Indeed is $P(Y_k>t)$. A formal integral is
	\begin{align}
		P(Y_k>t) &= n!\int_0^{1-t}\dots\int_0^{1-t}\int_t^1 dy_k dy_1\dots dy_n\\
			&= (1-t)^n
	\end{align}
	one might not simply apply 6.16, since $P(A_i\cap A_j)>0$ when $t<1/2$. However we can still use the intrinsic idea. That $P(A_i)=(1-t)^n$, and they are all equal, and they are random to $Y_k$, so average holds.\qed\\
	Notice T6.26 answer has the similar form, we ask (a) do any $m$ intervals with $Y_j>t\leq 1/m$, has the equility $P(\bigcap^m Y_j>t) = (1-mt)^n$? (b) what about exists $m$ $Y_j>t$?
	\begin{itemize}
		\item[(a)] we know $m=1,n-1$ holds, now consider normal case, since intervals are symmetry, we can focus on first $m$ intervals. By translation of origin to $(t,2t,...,mt)$, which holds order, and whose points in cube with length $1-mt$. Thus we get the probability $(1-mt)^n$. A formal integral proof is
		\begin{align}
			I =& n! \int_0^{1-mt}\dots\int_0^{1-mt-y_{m+2}-\dots}\int_t^{1-(m-1)t-y_{m+1}-\dots}\dots\int_t^{1-t-y_3-\dots-y_m-\dots}\int_t^{1-y_2-\dots-y_m-\dots} \\
				& dy_1dy_2\dots dy_m dy_{m+1}\dots dy_n\\
				=& (1-mt)^n
		\end{align}
		where holds for $m\leq n$. When $m=n+1$, add upper bound of each integral part $-t$, i.e.
		\begin{align}
			I = & n! \int_t^{1-nt}\dots\int_t^{1-2t-y_3-\dots}\int_t^{1-t-y_2-\dots} \\
				& dy_1 dy_2\dots dy_n\\
				=& (1-(n+1)t)^n
		\end{align}
		\item[(b)] for a special case $t>1/2$, $m=1$, recall clockwise semi-circle idea of 6.16, easy to get $P=(n+1)(1-t)^n$. For subgeneral case $1/(m+1)<t<1/m$, note there are exactly $m$ intervals, thus $P={n+1\choose m}(1-t)^n$. Then we can continue to $1/(m+2)<t<1/(m+1)$, to calculate contribution of $m+1$ and exactly $m$ ones. 
	\end{itemize}
	\item[S6.20]
	\begin{itemize}
		\item[(a)] $P = \frac{1/6 *1/5}{1/5} = 1/6$;
		\item[(b)] condition on $X_6>X_1$ or not, $P = \frac{1/6 *1/5 + 1/6* 1/2}{1/5} = 7/12$.
	\end{itemize}
\end{itemize}

\end{document}